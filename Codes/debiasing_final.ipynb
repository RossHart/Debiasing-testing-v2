{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from scipy.stats import binned_statistic, scoreatpercentile\n",
    "import pickle\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "from voronoi_2d_binning import voronoi_2d_binning\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "import imp # reload modules if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import binning\n",
    "import bin_debiasing\n",
    "import fit_debiasing\n",
    "import make_dictionaries\n",
    "import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib as mpl\n",
    "#from matplotlib import pyplot as plt\n",
    "# better-looking plots\n",
    "#plt.rcParams['font.family'] = 'serif'\n",
    "#plt.rcParams['figure.figsize'] = (10.0, 8)\n",
    "#plt.rcParams['font.size'] = 18\n",
    "#mpl.ticker.AutoLocator.default_params['nbins'] = 5\n",
    "#mpl.ticker.AutoLocator.default_params['prune'] = 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('output_files/') if os.path.isdir('output_files/') is False else None\n",
    "\n",
    "source_directory = params.source_directory\n",
    "full_sample = params.full_sample\n",
    "\n",
    "#save_directory = params.numpy_save_directory\n",
    "\n",
    "min_log_fv = -1.5\n",
    "max_log_fv = 0.01 # if >0, there is no upper limit to fitting fv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded galaxy data...\n",
      "Loaded questions...\n",
      "Loaded functions...\n"
     ]
    }
   ],
   "source": [
    "full_data = Table.read(source_directory + full_sample)\n",
    "#full_data = Table.read(source_directory + 'full_sample_debiased.fits')\n",
    "print('Loaded galaxy data...')\n",
    "questions = make_dictionaries.questions\n",
    "print('Loaded questions...')\n",
    "function_dictionary = make_dictionaries.function_dictionary\n",
    "print('Loaded functions...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce_sample(full_data,questions,question,p_cut=0.5,N_cut=5):\n",
    "    \n",
    "    # Get the reference sample from the previous data:\n",
    "    \n",
    "    previous_q = questions[question]['pre_questions']\n",
    "    previous_a = questions[question]['pre_answers']\n",
    "    \n",
    "    if previous_q != None:\n",
    "        \n",
    "        p_col = np.ones(len(full_data))\n",
    "        \n",
    "        for m in range(len(previous_q)):\n",
    "            p_col = p_col*(full_data[previous_q[m] + '_' + previous_a[m] + '_debiased_rh'])\n",
    "        N_col = (full_data[previous_q[-1] + '_' + previous_a[-1] + '_count'])\n",
    "        \n",
    "        select = (p_col > p_cut) & (N_col >= N_cut)\n",
    "        data_reduced = full_data[select]\n",
    "        print('{}/{} galaxies with p>{} and N>={}.'.format(len(data_reduced),\n",
    "                                                          len(full_data),p_cut,N_cut))\n",
    "    \n",
    "    else:\n",
    "        data_reduced = full_data.copy()\n",
    "        print('Primary question, so all {} galaxies used.'.format(len(data_reduced)))\n",
    "    \n",
    "    return data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bins(question,answer):\n",
    "    '''Get bins from if they have already been created from a \n",
    "    previous running of the debiasing'''\n",
    "    \n",
    "    bins = Table.read('output_files/'+ question + '/' + answer + '/bins.fits')\n",
    "    all_bins = Table.read('output_files/'+ question + '/' + answer + '/all_bins.fits')\n",
    "    vbins_table = Table.read('output_files/'+ question + '/' + answer + '/vbin_parameters.fits')\n",
    "    \n",
    "    vbins = bins['vbin']\n",
    "    zbins = bins['zbin']\n",
    "    zbins_coarse = bins['coarse_zbin']\n",
    "    vbins_all = all_bins['vbin']\n",
    "    zbins_all = all_bins['zbin']\n",
    "    zbins_coarse_all = all_bins['coarse_zbin']\n",
    "    \n",
    "    return vbins,zbins,zbins_coarse,vbins_all,zbins_all,zbins_coarse_all,vbins_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_01_range(dataset):\n",
    "    '''Returns proportion of 0s and 1s to be 'excluded' from the histograms'''\n",
    "    cf_low = np.sum(dataset == 0)/len(dataset)\n",
    "    N_1 = np.sum(dataset == 1)/len(dataset)\n",
    "    cf_high = 1-N_1\n",
    "    \n",
    "    return cf_low,cf_high\n",
    "\n",
    "\n",
    "def set_01_values(dataset,cf_low,cf_high):\n",
    "    '''Set the top and bottom ends to 0 and 1, to avoid 'false' rms values from 'undebiasable' values'''\n",
    "    \n",
    "    cf = np.linspace(0,1,len(dataset))\n",
    "    d_sorted = np.sort(dataset)\n",
    "    \n",
    "    indices = np.searchsorted(cf,[cf_low,cf_high])\n",
    "    indices = indices.clip(0,len(cf)-1)\n",
    "    \n",
    "    d_sorted[0:indices[0]] = 0\n",
    "    d_sorted[indices[1]:] = 1\n",
    "    \n",
    "    return d_sorted\n",
    "\n",
    "\n",
    "def histogram_fractions(data,hist_bins):\n",
    "    h,bin_edges = np.histogram(data,bins=hist_bins)\n",
    "    f = h/np.sum(h)\n",
    "    return f\n",
    "\n",
    "\n",
    "def get_rms(dataset,z_assignments,reference,hist_bins):\n",
    "    \n",
    "    ref_low,ref_high = get_01_range(reference)\n",
    "    \n",
    "    x = len(hist_bins) - 1\n",
    "    y = len(np.unique(z_assignments))\n",
    "    rms_array = np.zeros((x,y))\n",
    "\n",
    "    for n,z in enumerate(np.unique(z_assignments)):\n",
    "    \n",
    "        ref = reference.copy()\n",
    "        vl_deb = dataset[z_assignments == z]\n",
    "\n",
    "        deb_low,deb_high = get_01_range(vl_deb)\n",
    "        cf_low = np.max([ref_low,deb_low])\n",
    "        cf_high = np.min([ref_high,deb_high])\n",
    "    \n",
    "        vl_deb_01 = set_01_values(vl_deb,cf_low,cf_high)\n",
    "        ref_01 = set_01_values(ref,cf_low,cf_high)\n",
    "    \n",
    "        f_deb = histogram_fractions(vl_deb_01,hist_bins)\n",
    "        f_ref = histogram_fractions(ref_01,hist_bins)\n",
    "        \n",
    "        rms_array[:,n] = np.absolute(f_deb - f_ref)\n",
    "    \n",
    "    rms_value = np.mean(rms_array)  \n",
    "    \n",
    "    return rms_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_function(raw_data,debiased,question,answer):\n",
    "    \n",
    "    volume_ok = raw_data['in_volume_limit'] == 1\n",
    "    \n",
    "    vl  = raw_data[volume_ok][question + '_' + answer + '_weighted_fraction']\n",
    "    vl_bin = debiased['bin_method'][volume_ok]\n",
    "    vl_fit = debiased['fit_method'][volume_ok]\n",
    "    \n",
    "    redshifts = full_data['REDSHIFT_1'][volume_ok]\n",
    "    z_range = [np.min(redshifts),np.max(redshifts)]\n",
    "    z_vl_bins = np.linspace(z_range[0],z_range[1],11) # have 11 bins for now\n",
    "    z_vl_bins[0],z_vl_bins[-1] = [0,1] # ensure all data gets binned\n",
    "    z_assignments = np.digitize(redshifts,z_vl_bins)\n",
    "    \n",
    "    hist_bins = np.linspace(0,1,11)\n",
    "    hist_bins[0],hist_bins[-1] = [-1,2] # ensure all data gets binned\n",
    "\n",
    "    reference = vl[z_assignments == 1] # raw low-z for comparison\n",
    "    \n",
    "    rms_bin = get_rms(vl_bin,z_assignments,reference,hist_bins)\n",
    "    rms_fit = get_rms(vl_fit,z_assignments,reference,hist_bins)\n",
    "    \n",
    "    print('rms(bin) = {0:.3f}'.format(rms_bin))\n",
    "    print('rms(fit) = {0:.3f}'.format(rms_fit))\n",
    "    if rms_bin < rms_fit:\n",
    "        print('---> bin method selected')\n",
    "        debiased_values = debiased['bin_method']\n",
    "    else:\n",
    "        print('---> fit method selected')\n",
    "        debiased_values = debiased['fit_method']\n",
    "        \n",
    "    return debiased_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_and_debias(full_data,question,questions,answer,bins_exist=False,n_per_bin=100,coarse=False):\n",
    "    '''Set to 'coarse' to make the fitting only apply to the 'coarse binning'of 4 redshift bins per \n",
    "    voronoi bin rather than the fully binned data'''\n",
    "    \n",
    "    (os.mkdir('output_files/'+ question) if\n",
    "     os.path.isdir('output_files/'+ question) is False else None)\n",
    "    (os.mkdir('output_files/'+ question + '/' + answer) if\n",
    "     os.path.isdir('output_files/'+ question + '/' + answer) is False else None)\n",
    "    \n",
    "    data = reduce_sample(full_data,questions,question)\n",
    "    \n",
    "    if bins_exist == True:\n",
    "        vbins,zbins,zbins_coarse,vbins_all,zbins_all,zbins_coarse_all,vbins_table = get_bins(question,answer)\n",
    "        print('Bins obtained from previous iteration...')\n",
    "        \n",
    "    else:\n",
    "        vbins,zbins,zbins_coarse,vbins_all,zbins_all,zbins_coarse_all,vbins_table = binning.bin_data(data,\n",
    "                                                                                                     full_data,\n",
    "                                                                                                     question,\n",
    "                                                                                                     answer,\n",
    "                                                                                                     plot=False,\n",
    "                                                                                                     signal=n_per_bin)\n",
    "        \n",
    "    # Save the binning data  \n",
    "    bin_table = Table([vbins,zbins,zbins_coarse],names=('vbin','zbin','coarse_zbin'))\n",
    "    all_bin_table = Table([vbins_all,zbins_all,zbins_coarse_all],names=('vbin','zbin','coarse_zbin'))\n",
    "    bin_table.write('output_files/'+ question + '/' + answer + '/bins.fits',overwrite=True)\n",
    "    all_bin_table.write('output_files/'+ question + '/' + answer + '/all_bins.fits',overwrite=True)\n",
    "    vbins_table.write('output_files/'+ question + '/' + answer + '/vbin_parameters.fits',overwrite=True)\n",
    "\n",
    "    \n",
    "    debiased_bin = bin_debiasing.debias(data,full_data,vbins,zbins,vbins_all,zbins_all,question,answer)\n",
    "    debiased_fit,dout,fit_setup,zbins,fit_vbin_results = fit_debiasing.debias_by_fit(data,full_data,vbins,zbins,\n",
    "                                                                                     zbins_coarse,question,answer,\n",
    "                                                                                     function_dictionary,min_log_fv,\n",
    "                                                                                     coarse=coarse)\n",
    "    \n",
    "    volume_ok = data['in_volume_limit'] == 1    \n",
    "    vl_data = full_data[volume_ok]\n",
    "    vl_fit = debiased_fit[volume_ok]\n",
    "    vl_bin = debiased_bin[volume_ok]\n",
    "\n",
    "    debiased_table = Table([debiased_bin,debiased_fit],names=('bin_method','fit_method'))\n",
    "    debiased_table.write('output_files/'+ question + '/' + answer + '/debiased.fits',overwrite=True)\n",
    "    dout.write('output_files/'+ question + '/' + answer + '/fit_results.fits',overwrite=True)\n",
    "    pickle.dump(fit_setup,open('output_files/'+ question + '/' + answer + '/fit_setup.p', \"wb\" ))\n",
    "    \n",
    "    return debiased_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Question to be debiased: t11_arms_number\n",
      "Answer to be debiased: a36_more_than_4\n",
      "54804/219212 galaxies with p>0.5 and N>=5.\n",
      "Bin-accretion...\n",
      "120  initial bins.\n",
      "Reassign bad bins...\n",
      "24  good bins.\n",
      "Modified Lloyd algorithm...\n",
      "13  iterations.\n",
      "Unbinned pixels:  0  /  1711\n",
      "Fractional S/N scatter (%): 9.5599165469\n",
      "24 voronoi bins\n",
      "8.708333333333334 redshift bins per voronoi bin\n",
      "All bins fitted! 21.268635034561157s in total\n",
      "chisq(logistic) = 0.008176855508237336\n",
      "All bins fitted! 27.35903811454773s in total\n",
      "chisq(exp. power) = 1.5396379342522e-05\n",
      "All bins fitted! 16.85997700691223s in total\n",
      "All bins fitted! 16.96443510055542s in total\n",
      "rms(bin) = 0.012\n",
      "rms(fit) = 0.006\n",
      "---> fit method selected\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "question_order = ['t01_smooth_or_features',\n",
    "                  't02_edgeon',\n",
    "                  't04_spiral',\n",
    "                  't11_arms_number']\n",
    "'''\n",
    "\n",
    "imp.reload(fit_debiasing)\n",
    "\n",
    "question_order = ['t11_arms_number']\n",
    "\n",
    "for question in question_order:\n",
    "    #answers = questions[question]['answers']\n",
    "    answers = ['a36_more_than_4']\n",
    "    for answer in answers:\n",
    "        \n",
    "        #bins_exist = os.path.isfile('output_files/'+ question + '/' + answer + '/bins.fits')\n",
    "        bins_exist = False\n",
    "        \n",
    "        print('----------------------------------')\n",
    "        print('Question to be debiased:',question)\n",
    "        print('Answer to be debiased:',answer)\n",
    "        \n",
    "        debiased = bin_and_debias(full_data,question,questions,answer,\n",
    "                                  bins_exist=bins_exist,n_per_bin=50,coarse=True) # set to coarse to test method.\n",
    "        \n",
    "        deb_vals = choose_best_function(full_data,debiased,question,answer)\n",
    "        full_data[question + '_' + answer + '_debiased_rh'] = deb_vals\n",
    "        \n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debiased_columns = ['t01_smooth_or_features_a01_smooth_debiased_rh',\n",
    "                    't01_smooth_or_features_a02_features_or_disk_debiased_rh',\n",
    "                    't01_smooth_or_features_a03_star_or_artifact_debiased_rh',\n",
    "                    't02_edgeon_a04_yes_debiased_rh',\n",
    "                    't02_edgeon_a05_no_debiased_rh',\n",
    "                    't04_spiral_a08_spiral_debiased_rh',\n",
    "                    't04_spiral_a09_no_spiral_debiased_rh',\n",
    "                    't11_arms_number_a31_1_debiased_rh',\n",
    "                    't11_arms_number_a32_2_debiased_rh',\n",
    "                    't11_arms_number_a33_3_debiased_rh',\n",
    "                    't11_arms_number_a34_4_debiased_rh',\n",
    "                    't11_arms_number_a36_more_than_4_debiased_rh',\n",
    "                    't11_arms_number_a37_cant_tell_debiased_rh']\n",
    "\n",
    "debiased_values = full_data[debiased_columns]\n",
    "debiased_values.write(source_directory + 'debiased_values.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data.write(source_directory + 'full_sample_debiased.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fit_debiasing' from '/Users/rosshart/Documents/Github_repos/Debiasing-testing-v2/Codes/fit_debiasing.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(fit_debiasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
